\documentclass{article}%ctex
\input{~/code/math_commands.tex}




\title{\huge Lecture 3 \\
\normalsize}
\author{Xuanxi Zhang}
\begin{document}
\maketitle
\section*{Topics}
\begin{itemize}
    \item Conditional Independence $\leftrightarrow$ Graphical Representation
    \item Existence and Unicity
    \item Independence Maps and D-separation
\end{itemize}

\textbf{Recall from Lecture 1:} Brute-force approach to estimate a joint probability distribution does not scale — need to introduce structure!

\textbf{Recall from Lecture 2:} First attempt at structure: focus on first two moments and disregard the rest $\rightarrow$ Gaussian Estimation. Efficient in high dimensions, but limited power to model many important phenomena.

\textbf{Alternative route to structure: Statistical Independence.}

\section{Probability Basic}

\subsection{Independence}
Let $(X, Y) \sim P$ with marginals $P_X$ and $P_Y$.

\begin{itemize}
    \item $X$ and $Y$ are independent if and only if:
    \[
    P = P_X \otimes P_Y
    \]
    \item When $P$ has density $p(x,y)$:
    \[
    p(x,y) = p_X(x) p_Y(y)
    \]
\end{itemize}

\textbf{Key Property}
The marginals in the right-hand side (RHS) are of lower dimension.

\paragraph{Example: Hypercube}
Consider again the hypercube $\mathbb{H}^d = \{ \pm 1 \}^d$.

Suppose that $\{ X_1, \dots, X_{d/2} \}$ is independent from $\{ X_{d/2+1}, \dots, X_d \}$. Then,

\[
P_{\theta}(X_1, \dots, X_d) = P_{\theta_1}(X_1, \dots, X_{d/2}) \cdot P_{\theta_2}(X_{d/2+1}, \dots, X_d)
\]

where $\theta = (\theta_1, \theta_2)$.
Given $n$ i.i.d. observations $x^1, \dots, x^n$, the MLE for $\theta$ is:

\[
\frac{1}{n} \sum_{i=1}^{n} \log P_{\theta}(x^i) = \frac{1}{n} \sum_{i} \log P_{\theta_1}(x^i_{1:d/2}) + \frac{1}{n} \sum_{i} \log P_{\theta_2}(x^i_{d/2+1:d})
\]

\[
= \mathcal{L}(\theta_1) + \mathcal{L}(\theta_2)
\]

This corresponds to \textbf{multivariate models in} $\mathbf{d/2}$ \textbf{dimensions}.



\subsection{Conditional Independence}

\[
p(X_3 \mid X_1, X_2) = p(X_3 \mid X_1) \quad \text{(or equivalently, } p(X_3 \mid X_1, X_2) \text{)}
\]

This means that \( X_2 \) and \( X_3 \) are \textbf{conditionally independent} given \( X_1 \).

\paragraph{Conditional Independence Property}
\textbf{Assume} \( p(X_2 \mid X_1) > 0 \) for all \( X_1, X_2 \). Then:
\[
p(X_3 \mid X_1, X_2) = p(X_3 \mid X_1) \iff p(X_2, X_3 \mid X_1) = p(X_2 \mid X_1) \cdot p(X_3 \mid X_1)
\]
\textbf{Proof:}
\[
\Rightarrow \text{:}\ \  p(X_3, X_2 \mid X_1) = p(X_2 \mid X_1) \cdot p(X_3 \mid X_1, X_2)
= p(X_2 \mid X_1) \cdot p(X_3 \mid X_1)
\]
\[
\Leftarrow \text{:}\ \  p(X_3 \mid X_1, X_2) = \frac{p(X_2, X_3 \mid X_1)}{p(X_2 \mid X_1)} = p(X_3 \mid X_1).
\]
\textbf{Important Fact:} Unconditional independence does not imply conditional independence:
\[
X \perp Y \not\Rightarrow X \perp Y \mid Z
\]
\[
X \perp Y \not\Leftarrow X \perp Y \mid Z
\]
\textbf{Example 1:}
\begin{itemize}
    \item \( X \): Alice is working late at night.
    \item \( Y \): Bob is working late at night.
    \item \( Z \): There is a conference deadline tomorrow!
\end{itemize}
\textbf{Example 2:}
\begin{itemize}
    \item \( X \): Parent 1 is left-handed.
    \item \( Y \): Parent 2 is left-handed.
    \item \( Z \): The child is left-handed.
\end{itemize}

\section{Bayes Factorization and Bayesian Networks}
For a joint distribution \( p \in \mathcal{P}(\mathcal{X}^d) \), we have:
\[
p \in \mathcal{P}(\mathcal{X}^d), \quad p(x_1, \dots, x_d) = \prod_{i=1}^{d} p(x_i \mid x_{1:i-1})
\]
The right-hand side (RHS) can be represented using a \textbf{directed graph}:
\[
G = (V, E)
\]
where the \textbf{vertex set} is \( V = \{X_i\}_{i=1}^{d} \). And edges are induced by the \textbf{conditioning} in the factorization:
\[
p(x_i \mid \{x_j\}_{j \in A(i)}) \Rightarrow A(i) = \{ j \in V \mid j \to i \in E \}
\]
properties of the graph: The \textbf{parents} of each node encode the conditioning set. Notice that the \textbf{in-degree} \( D_i \) of each node:
\[
D_i = | A(i) |
\]
determines the dimension of the associated conditional probability model. Thus, when \( \max_i D(i) = O(d) \), we are \textbf{cursed by dimension}. In this example, we have, in fact, the \textbf{densest possible} graph.

To alleviate the \textbf{Curse of Dimensionality (CoD)}, we need to \textbf{sparsify} the graph by removing unnecessary edges so that the \textbf{degree} is reduced.
\[
p(X_i \mid X_{1:i-1}) = p(X_i \mid X_{A(i)})
\]
where the conditioning set is smaller, with:
\[
|A(i)| < i - 1.
\]

\subsection{Bayesian Networks}

\begin{itemize}
    \item First, pick an \textbf{ordering} of variables and consider the model:
          \[
          P(X_1, \dots, X_d) = \prod_{i=1}^{d} P(X_i \mid X_{A(i)})
          \]
          with \( A(i) \subseteq \{1, \dots, i-1\} \).
    \item We saw how to represent this model using a \textbf{Directed Acyclic Graph (DAG)}:
          \[
          G = (V, E),
          \]
          where \( (i \to j) \in E \iff i \in A(j) \).
\end{itemize}

\paragraph{Open Questions}
\begin{itemize}
    \item What are the best \textbf{algorithms} for inference and querying the probability model?
    \item How accurate is the \textbf{translation} between joint probability distributions and DAGs?
\end{itemize}

\textit{To be discussed later.}

\subsection{Example: Naïve Bayes Classifier}
Consider discrete covariates \( X = (X_1, \dots, X_d) \) and a label \( Y \).

For example:
\begin{itemize}
    \item \( Y = \mathbf{1}(\text{email is spam}) \)
    \item \( X_i = \mathbf{1}(\text{word}_i \in \text{email}) \)
\end{itemize}

\textbf{Naïve Bayes Model:}
\[
p(X_1, \dots, X_d, Y) = p(Y) \cdot \prod_{i=1}^{d} p(X_i \mid Y)
\]

Spam is detected given \( X \) if:

\[
P(Y=1) \prod_{i=1}^{d} P(X_i \mid Y=1)
\]

is greater than:

\[
P(Y=0)  \prod_{i=1}^{d} P(X_i \mid Y=0).
\]



\section{Directed Acyclic Graphs (DAGs)}
We can see that the graph build from bayes factorization is a DAG.

\textbf{Question:} Why must the graph be acyclic?

\textbf{Answer:}
\begin{itemize}
    \item A Bayes decomposition starts with an \textbf{ordering of variables}, (which can be arbitrary).
    \item This defines a \textbf{topological ordering} in \( G \), such that:
          \[
          (i, j) \in E \Rightarrow j > i.
          \]
    \item It follows that \( G \) \textbf{cannot have cycles}. Suppose a cycle exists:
          \[
          i_1 \to i_2 \to i_3 \to \dots \to i_k \to i_1
          \]
          but this contradicts the topological ordering, where:
          \[
          i_1 < i_2 < i_3 < \dots < i_k < i_1,
          \]
          which is impossible.
\end{itemize}








\section{Mapping Between Bayesian Probability and DAGs}

\begin{definition}
    Let \( P \) be a probability distribution on \( \mathcal{X} \) (input space). We define \( I(P) \) as the set of all \textbf{conditional independence} relations satisfied by \( P \), i.e.,
\[
X \perp Y \mid Z, \quad Z \subseteq \mathcal{X}, \quad X, Y \in \mathcal{X}.
\]
\end{definition}
Ideally, we would like the \textbf{graphical representation} to capture \( I(P) \).
\begin{definition}[factorization]
    Let \( G \) be a Directed Acyclic Graph (DAG) over variables \( X_1, \dots, X_d \). We say that \( P \) \textbf{factorizes} over \( G \) if:
    \[
    P(X_1, \dots, X_d) = \prod_{i=1}^{d} P(X_i \mid X_{A(i)})
    \]
    where \( A(i) \) represents the \textbf{parents} of node \( i \) in \( G \).    
\end{definition}
\begin{definition}[Independence Map]
    A DAG \( G \) is an \textbf{Independence Map} for \( P \) if:

    \[
    I_l(G) = \left\{ X_i \perp \text{Non-Descendants}(X_i) \mid X_{A(i)} \right\}, \quad i = 1, \dots, d
    \]
    
    is satisfied under \( P \). This means that \( I_l(G) \) contains the \textbf{local independencies}.
\end{definition}

\begin{itemize}
    \item \textbf{Key Fact}
    \[
    P \text{ factorizes according to } G \iff G \text{ is an independence map for } P.
    \]
    
    \textbf{Proof: }
    This follows from \textbf{F \& K Theorems 3.1 and 3.2} (extending our previous argument).
    \item In other words, knowing that \( P \) \textbf{factorizes} according to \( G \) provides an explicit list of \textbf{local conditional independencies}, meaning:

    \[
    I_l(G) \subseteq I(P).
    \]
    \item \textbf{Can we also obtain non-local conditional independencies from \( G \)?}

\end{itemize}




\section{D-Separation}
\textbf{Goal:} Given a DAG \( G \) and variables \( X, Y \in \mathcal{X} \), determine whether:

\[
X \perp Y \mid \gZ, \quad \gZ \subseteq \mathcal{X}
\]

\textbf{Main Idea:} Determine whether there exists an \textbf{active path} between \( X \) and \( Y \) when \( \gZ \) is observed.


\begin{definition}[D-Separation]
    Let $X, Y$, and $Z$ be disjoint sets of nodes in a DAG. We say $X$ and $Y$ are d-separated by $Z$ if every path from any node in $X$ to any node in $Y$ is blocked by $Z$.

A path is blocked by $Z$ if at least one triple along the path:
\begin{enumerate}
    \item Chain or Fork: $A \rightarrow B \rightarrow C$ or $A \leftarrow B \rightarrow C$. The path is blocked if the middle node $B$ is \textbf{in} $Z$.
    \item Collider: $A \rightarrow B \leftarrow C$, The path is blocked if the middle node $B$ is \textbf{not in} $Z$ and none of its descendants are in $Z$.
\end{enumerate}

If a path is not blocked, we sometimes say it is "active." If exist a path between $X$ and $Y$ is active, then $X$ and $Y$ are not d-separated.

Given a set \( \gZ \). We define:
    \[
    I(G) = \left\{ (X \perp Y \mid \gZ) \mid X, Y \text{ are d-separated with respect to } \gZ \right\}
    \]
\end{definition}




\textbf{Claim}
If a distribution \( P \) factorizes over \( G \), then:
\[
I(G) \subseteq I(P).
\]
In other words, any \textbf{conditional independence} that we extract from \textbf{d-separation} is satisfied by \( P \).

Proof: To be covered in the next lecture, using undirected graphs.

\textbf{Prop:}
For any distribution \( P \) that factorizes over \( G \), except for a set of measure zero, we have:

\[
I(G) = I(P).
\]

(This accounts for corner cases from degenerate distributions.)

Note that this does \textbf{NOT} imply a tight equivalence between \textbf{DAGs} and \textbf{probability models}. Under the concept of \textbf{equivalence in independence relation}, we have:
\begin{enumerate}
    \item Non-Uniqueness of DAG Representations: We can have two \textbf{non-isomorphic} DAGs \( G_1 \) and \( G_2 \) (i.e., \( G_1 \neq G_2 \)), such that:
    \[
    I(G_1) = I(G_2).
    \]. For example, consider:
    \[
    X \rightarrow Y \rightarrow Z\ \  \text{and} \ \ X \leftarrow Y \rightarrow Z.
    \]
    \item Non-Existence of DAG Representations: For some distributions \( P \), ther does not exist a DAG \( G \) such that \( I(G) = I(P) \). For example, consider:
    \[
    X, Y \sim \text{Bernoulli}(1/2) \text{ independently}, \quad Z = X \oplus Y.
    \]
    The joint distribution \( (X, Y, Z) \) is \textbf{exchangeable} and \textbf{not adapted} to a directed graphical structure.
\end{enumerate}



\end{document}




\subsection*{Example}
Consider the following variables:
\begin{itemize}
    \item \( G \): Gender
    \item \( A \): Age
    \item \( MH \): Mother’s Height
    \item \( FH \): Father’s Height
    \item \( H \): Child’s Height
    \item \( W \): Child’s Weight
\end{itemize}

\subsection*{Graphical Representation}
The Bayesian network structure implies the following joint probability decomposition:

\[
P(A, G, H, W, MH, FH) = P(G) \cdot P(A) \cdot P(MH) \cdot P(FH) \cdot P(H \mid G, MH, FH, A) \cdot P(W \mid G, A)
\]

\subsection*{Topological Ordering}
The topological ordering of the nodes in the network helps in identifying conditional independences.

\subsection*{Reading Off Conditional Independences}
From the structure of the graph, we can infer the following conditional independences:
\[
X_1 \perp X_2, \quad X_1 \perp X_3, \quad X_2 \perp X_3, \quad X_3 \perp X_4
\]
\[
X_5 \perp X_6 \mid X_4
\]

\subsection*{Key Property}
A variable is \textbf{independent} of its \textbf{non-descendants}, given its \textbf{parents}.
\section*{Takeaway So Far}
We have two different notions of \textbf{separation}:
\begin{itemize}
    \item A \textbf{probabilistic} one: \( X \perp Y \mid Z \) (between random variables).
    \item A \textbf{topological} one between nodes in a graph.
\end{itemize}

\textbf{Question:} Given a probabilistic model, can we always build a DAG \( G \) such that these two notions agree?
